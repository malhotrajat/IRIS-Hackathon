{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IRIS Online Review Prediction Model\n",
    "The task of this hackathon challenge is to create a predictive model to classify online electronics reviews into five categories, ranking from 1 (lowest) to 5 (highest). Both train sets and test sets are provided. We recommend the following guidelines in preparing the data set, training/testing the model, and applying it to the final test set. \n",
    "\n",
    "1. Resampling the dataset into more balanced dataset due to the nature of online reviews.\n",
    "2. Feature engineering - cleansing and expanding the data set into a set of features that text mining algorithms expect.\n",
    "3. Testing the model by cross validation.\n",
    "4. Apply the model to a test set and report the results for scoring.\n",
    "\n",
    "This document aims at explaing step by step the concepts and procedures in the making of the predictive model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading in the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.mlab as mlab\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To read the entire training set into memory and train the model based on all of it has two major downsides:\n",
    "1. The sheer volume (1.6 million rows) of the data would incur high computational cost. Given the time constraint in the hackathon, such computational load is inhibitive.\n",
    "2. The data set is unbalanced by nature. Most reviews collected are positive (4-5), while neutral and below (1-3) are much rarer. Training the model on such data set will likely lead to biased prediction. The histogram shows the unbalanced nature of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "train = pd.read_csv('train.csv')\n",
    "train = train.loc[1:5000,:]\n",
    "num_bins = 5\n",
    "n, bins, patches = plt.hist(train['Rating'], num_bins, facecolor='blue', alpha=0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resampling code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by counting the number of words and number of characters in each reviews, as well as the average length of words. These three new features are appended to the training data set as new features. The techniques are self-explanatory in the code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ReviewText</th>\n",
       "      <th>word_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I'm a professional OTR truck driver, and I bou...</td>\n",
       "      <td>446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Well, what can I say.  I've had this unit in m...</td>\n",
       "      <td>888</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Not going to write a long review, even thought...</td>\n",
       "      <td>449</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I've had mine for a year and here's what we go...</td>\n",
       "      <td>202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>I am using this with a Nook HD+. It works as d...</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          ReviewText  word_count\n",
       "1  I'm a professional OTR truck driver, and I bou...         446\n",
       "2  Well, what can I say.  I've had this unit in m...         888\n",
       "3  Not going to write a long review, even thought...         449\n",
       "4  I've had mine for a year and here's what we go...         202\n",
       "5  I am using this with a Nook HD+. It works as d...          22"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['word_count'] = train['ReviewText'].apply(lambda x: len(str(x).split(\" \")))\n",
    "train[['ReviewText','word_count']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number of characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ReviewText</th>\n",
       "      <th>char_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I'm a professional OTR truck driver, and I bou...</td>\n",
       "      <td>2175.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Well, what can I say.  I've had this unit in m...</td>\n",
       "      <td>4607.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Not going to write a long review, even thought...</td>\n",
       "      <td>2246.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I've had mine for a year and here's what we go...</td>\n",
       "      <td>1076.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>I am using this with a Nook HD+. It works as d...</td>\n",
       "      <td>109.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          ReviewText  char_count\n",
       "1  I'm a professional OTR truck driver, and I bou...      2175.0\n",
       "2  Well, what can I say.  I've had this unit in m...      4607.0\n",
       "3  Not going to write a long review, even thought...      2246.0\n",
       "4  I've had mine for a year and here's what we go...      1076.0\n",
       "5  I am using this with a Nook HD+. It works as d...       109.0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['char_count'] = train['ReviewText'].str.len() ## this also includes spaces\n",
    "train[['ReviewText','char_count']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Average Word Length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ReviewText</th>\n",
       "      <th>avg_word</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I'm a professional OTR truck driver, and I bou...</td>\n",
       "      <td>4.051522</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Well, what can I say.  I've had this unit in m...</td>\n",
       "      <td>4.397163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Not going to write a long review, even thought...</td>\n",
       "      <td>4.004454</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I've had mine for a year and here's what we go...</td>\n",
       "      <td>4.331683</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>I am using this with a Nook HD+. It works as d...</td>\n",
       "      <td>4.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          ReviewText  avg_word\n",
       "1  I'm a professional OTR truck driver, and I bou...  4.051522\n",
       "2  Well, what can I say.  I've had this unit in m...  4.397163\n",
       "3  Not going to write a long review, even thought...  4.004454\n",
       "4  I've had mine for a year and here's what we go...  4.331683\n",
       "5  I am using this with a Nook HD+. It works as d...  4.000000"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def avg_word(sentence):\n",
    "  #print(sentence)\n",
    "  sentence = str(sentence)\n",
    "  words = sentence.split()\n",
    "  return (sum(len(word) for word in words)/len(words))\n",
    "\n",
    "train['avg_word'] = train['ReviewText'].apply(lambda x: avg_word(x))\n",
    "train[['ReviewText','avg_word']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number of Stop Words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the major forms of pre-processing is to filter out useless data. In natural language processing, useless words (data), are referred to as stop words. A stop word is a commonly used word (such as “the”, “a”, “an”, “in”). The algorithm below identifies such words in English and count the number of occurences in each review. Similarly, the number of special characters/punctuations as well as numerics is also recorded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ReviewText</th>\n",
       "      <th>stopwords</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I'm a professional OTR truck driver, and I bou...</td>\n",
       "      <td>177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Well, what can I say.  I've had this unit in m...</td>\n",
       "      <td>362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Not going to write a long review, even thought...</td>\n",
       "      <td>196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I've had mine for a year and here's what we go...</td>\n",
       "      <td>86</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>I am using this with a Nook HD+. It works as d...</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          ReviewText  stopwords\n",
       "1  I'm a professional OTR truck driver, and I bou...        177\n",
       "2  Well, what can I say.  I've had this unit in m...        362\n",
       "3  Not going to write a long review, even thought...        196\n",
       "4  I've had mine for a year and here's what we go...         86\n",
       "5  I am using this with a Nook HD+. It works as d...          8"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "# nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "#print(train['ReviewText'])\n",
    "#train['ReviewText',:] = str(train['ReviewText',:])\n",
    "\n",
    "train['stopwords'] = train['ReviewText'].apply(lambda x: len([x for x in str(x).split() if x in stop_words]))\n",
    "train[['ReviewText','stopwords']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number of Special Characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ReviewText</th>\n",
       "      <th>hastags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I'm a professional OTR truck driver, and I bou...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Well, what can I say.  I've had this unit in m...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Not going to write a long review, even thought...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I've had mine for a year and here's what we go...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>I am using this with a Nook HD+. It works as d...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          ReviewText  hastags\n",
       "1  I'm a professional OTR truck driver, and I bou...        0\n",
       "2  Well, what can I say.  I've had this unit in m...        0\n",
       "3  Not going to write a long review, even thought...        0\n",
       "4  I've had mine for a year and here's what we go...        0\n",
       "5  I am using this with a Nook HD+. It works as d...        0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['hastags'] = train['ReviewText'].apply(lambda x: len([x for x in str(x).split() if x.startswith('#')]))\n",
    "train[['ReviewText','hastags']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number of Numerics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ReviewText</th>\n",
       "      <th>numerics</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I'm a professional OTR truck driver, and I bou...</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Well, what can I say.  I've had this unit in m...</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Not going to write a long review, even thought...</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I've had mine for a year and here's what we go...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>I am using this with a Nook HD+. It works as d...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          ReviewText  numerics\n",
       "1  I'm a professional OTR truck driver, and I bou...         7\n",
       "2  Well, what can I say.  I've had this unit in m...         7\n",
       "3  Not going to write a long review, even thought...         6\n",
       "4  I've had mine for a year and here's what we go...         1\n",
       "5  I am using this with a Nook HD+. It works as d...         0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['numerics'] = train['ReviewText'].apply(lambda x: len([x for x in str(x).split() if x.isdigit()]))\n",
    "train[['ReviewText','numerics']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making every review lower case\n",
    "To further make the data set ready for algorithms, all words are convered to lower case and punctuations removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    i'm a professional otr truck driver, and i bou...\n",
       "2    well, what can i say. i've had this unit in my...\n",
       "3    not going to write a long review, even thought...\n",
       "4    i've had mine for a year and here's what we go...\n",
       "5    i am using this with a nook hd+. it works as d...\n",
       "Name: ReviewText, dtype: object"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['ReviewText'] = train['ReviewText'].apply(lambda x: \" \".join(x.lower() for x in str(x).split()))\n",
    "train['ReviewText'].head()\n",
    "# Translate here later\n",
    "#translateDiction = pd.read_csv('Contractions.csv')\n",
    "#print(translateDiction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing Punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    im a professional otr truck driver and i bough...\n",
       "2    well what can i say ive had this unit in my tr...\n",
       "3    not going to write a long review even thought ...\n",
       "4    ive had mine for a year and heres what we got ...\n",
       "5    i am using this with a nook hd it works as des...\n",
       "Name: ReviewText, dtype: object"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['ReviewText'] = train['ReviewText'].str.replace('[^\\w\\s]','')\n",
    "train['ReviewText'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing Stop Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    im professional otr truck driver bought tnd 70...\n",
       "2    well say ive unit truck four days prior garmin...\n",
       "3    going write long review even thought unit dese...\n",
       "4    ive mine year heres got tries route non truck ...\n",
       "5    using nook hd works described hd picture samsu...\n",
       "Name: ReviewText, dtype: object"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop = stopwords.words('english')\n",
    "train['ReviewText'] = train['ReviewText'].apply(lambda x: \" \".join(x for x in str(x).split() if x not in stop))\n",
    "train['ReviewText'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Common word removal (Top N, user discretion required)\n",
    "Certain words are common across the entire data set that are not stop words. For example for hard drive product reviews, the word \"hard drive\" is certainly not a stop word but is prevalent everywhere, and do not process any differentiating value when it comes to predicting ratings. These words are thus removed from the data set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq = pd.Series(' '.join(train['ReviewText']).split()).value_counts()[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    im professional otr truck driver bought tnd 70...\n",
       "2    say ive unit truck four days prior garmin 755t...\n",
       "3    going write long review even thought unit dese...\n",
       "4    ive mine year heres got tries route non truck ...\n",
       "5    using nook hd works described hd picture samsu...\n",
       "Name: ReviewText, dtype: object"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freq = list(freq.index)\n",
    "train['ReviewText'] = train['ReviewText'].apply(lambda x: \" \".join(x for x in x.split() if x not in freq))\n",
    "train['ReviewText'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rare Words removal\n",
    "Similar to common word removal, rare words removal is required as well. Rare words tend to create exceptions in machine learning models that ultimately add unwanted bias to the prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq = pd.Series(' '.join(train['ReviewText']).split()).value_counts()[-10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    im professional otr truck driver bought tnd 70...\n",
       "2    say ive unit truck four days prior garmin 755t...\n",
       "3    going write long review even thought unit dese...\n",
       "4    ive mine year heres got tries route non truck ...\n",
       "5    using nook hd works described hd picture samsu...\n",
       "Name: ReviewText, dtype: object"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freq = list(freq.index)\n",
    "train['ReviewText'] = train['ReviewText'].apply(lambda x: \" \".join(x for x in x.split() if x not in freq))\n",
    "train['ReviewText'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simple spelling correction is conducted using off-the-shelf tools."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    in professional or truck driver bought and 700...\n",
       "Name: ReviewText, dtype: object"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from textblob import TextBlob\n",
    "train['ReviewText'][:1].apply(lambda x: str(TextBlob(x).correct()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization\n",
    "Tokenization is the procedure where entire sentences are broken into minimal units of words for analysis. Numerous research recommendations exist on the rules and algorithms to convert long texts to tokens. The TextBlob package is used to perform simple analysis. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WordList(['going', 'write', 'long', 'review', 'even', 'thought', 'unit', 'deserves', 'ive', 'driven', '1mil', 'miles', 'done', 'routing', 'pretty', 'know', 'whats', 'fastest', 'shortest', 'using', 'basic', 'garmin', 'past', 'three', 'years', 'gps', 'unit', 'theyll', 'trouble', 'let', 'really', 'excited', 'unit', 'due', 'size', 'features', 'allot', 'grafics', 'screen', 'info', 'thats', 'usefull', 'basic', 'item', 'lacking', 'gps', 'tracking', 'gave', 'unit', 'allot', 'leadway', 'mistakes', 'due', 'fact', 'allot', 'cool', 'stuff', 'ability', 'track', 'route', 'even', 'close', 'basic', 'garmin', 'could', 'due', 'prossesor', 'installed', 'tnd', '700', '10', 'years', 'old', 'example', 'needed', 'make', 'simple', 'route', 'change', 'ie', 'town', 'next', 'street', 'due', 'fact', 'couldnt', 'make', 'turn', 'street', 'blocked', 'take', 'tnd', '700', 'upwards', '45', 'seconds', 'minute', 'half', 'reroute', 'im', 'sitting', 'stop', 'light', 'waiting', 'directions', 'waiting', 'long', 'cars', 'backside', 'didnt', 'make', 'happy', 'problem', 'happened', 'evertime', 'reroute', 'weather', 'simple', 'street', 'change', 'major', 'highway', 'change', 'also', 'time', 'turned', 'unit', 'take', 'twice', 'long', 'boot', 'least', 'dozen', 'times', 'week', 'put', 'wrong', 'roads', 'made', 'wrong', 'turn', 'got', 'self', 'lostie', 'take', 'left', 'xyz', 'street', 'ones', 'drive', 'way', 'turn', 'left', '800', 'yards', 'turn', 'less', '10', 'feet', 'away', 'might', 'think', 'conjested', 'city', 'situation', 'might', 'little', 'mixed', 'country', 'twice', 'put', '126', 'bridges', 'im', '136', 'course', 'made', 'sure', 'truck', 'setting', 'supposed', 'also', 'updated', 'os', 'versionvia', 'rand', 'mcnally', 'expected', 'alot', 'unit', 'got', 'unit', 'field', 'tested', 'people', 'drive', 'allot', 'many', 'route', 'mistakes', 'going', 'back', 'basic', 'garmin', 'isnt', 'complaints', 'three', 'friends', 'bought', 'unit', 'complaints', 'us', 'returned', 'units'])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# nltk.download('punkt')\n",
    "\n",
    "TextBlob(train['ReviewText'][3]).words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming\n",
    "To make the tokens even more stardardized across comments, one scenario that is common across English is plural form, past tense, present perfect tense etc. Similar to humans being able to use these information to express certain connotations, it is currently still a work-in-progress item for text mining to extract information from these connotations, and the lemmatization section below offers more details. Thus the PorterStemmer package is used to convert stemming-affected words back to their original form by removing pre or suffix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    im profession otr truck driver bought tnd 700 ...\n",
       "Name: ReviewText, dtype: object"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "st = PorterStemmer()\n",
    "train['ReviewText'][:1].apply(lambda x: \" \".join([st.stem(word) for word in x.split()]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatization\n",
    "emmatization, on the other hand, takes into consideration the morphological analysis of the words. To do so, it is necessary to have detailed dictionaries which the algorithm can look through to link the form back to its lemma."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    im professional otr truck driver bought tnd 70...\n",
       "2    say ive unit truck four day prior garmin 755t ...\n",
       "3    going write long review even thought unit dese...\n",
       "4    ive mine year here got try route non truck rou...\n",
       "5    using nook hd work described hd picture samsun...\n",
       "Name: ReviewText, dtype: object"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# nltk.download('wordnet')\n",
    "from textblob import Word\n",
    "train['ReviewText'] = train['ReviewText'].apply(lambda x: \" \".join([Word(word).lemmatize() for word in x.split()]))\n",
    "train['ReviewText'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parts of Speech Tagging\n",
    "In corpus linguistics, part-of-speech tagging (POS tagging or PoS tagging or POST), also called grammatical tagging or word-category disambiguation, is the process of marking up a word in a text (corpus) as corresponding to a particular part of speech. In particular, POS tagging in this hackathon specifically targets different parts of the speach into verbs, nouns, adjectives etc. There are multiple methods for POS such as lexical based, rule-based, probabilistic, and deep learning methods. Python provides a dictionary data type that enables convenient POS tagging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nltk.download('averaged_perceptron_tagger')\n",
    "from nltk import word_tokenize, pos_tag, pos_tag_sents\n",
    "texts = train['ReviewText'].tolist()\n",
    "tagged_texts = pos_tag_sents(map(word_tokenize, texts))\n",
    "\n",
    "train['POS'] = tagged_texts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Named Entity Recognition\n",
    "Named entities are definite noun phrases that refer to specific types of individuals, such as organizations, persons, dates, and so on. The goal of a named entity recognition (NER) system is to identify all textual mentions of the named entities. This can be broken down into two sub-tasks: identifying the boundaries of the NE, and identifying its type. Named entity recognition is frequently a prelude to identifying relations in Information Extraction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nltk.download('words')\n",
    "from nltk import word_tokenize, pos_tag, ne_chunk\n",
    "a = [0] * len(train['ReviewText'])\n",
    "for i in range(len(train['ReviewText'])):\n",
    "    temp = word_tokenize(str(train.iloc[i,0]))\n",
    "    a[i] = str(pos_tag(temp))\n",
    "\n",
    "temp2 = ne_chunk(a)\n",
    "#len(temp2)\n",
    "train['ner']=ne_chunk(a)\n",
    "print(train['ner'].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### N-Grams\n",
    "N-Grams are basically a set of co-occuring words within a given window and when computing the n-grams you typically move one word forward. There is a computational time vs accuracy trade off here, but a quick trial reveals that a 2-Grams works well in this data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['Ngrams'] = train['ReviewText'].apply(lambda x: TextBlob(x).ngrams(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bag of Words\n",
    "After preprocessing in the previous steps, the sentences are further broken down into each individual words. Sequence is no longer preserved in this step. Rather, multiplicity is calculated as a feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "bow = CountVectorizer(max_features=1000, lowercase=True, ngram_range=(1,2),analyzer = \"word\")\n",
    "train_bow = bow.fit_transform(train['ReviewText'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(train_bow).to_csv(\"train_bag_of_words.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.to_csv(\"train_Features.csv\",index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the entire feature engineering process, the feature set now looks like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
